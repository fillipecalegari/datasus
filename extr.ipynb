{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c41583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftplib import FTP\n",
    "import io\n",
    "import datasus_dbc\n",
    "from dbfread import DBF\n",
    "from dbfread import DBF, FieldParser \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222dbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFieldParser(FieldParser):\n",
    "    def parseD(self, field, data):\n",
    "        try:\n",
    "            return super().parseD(field, data)\n",
    "        except Exception:\n",
    "            return None   # datas inválidas viram None\n",
    "\n",
    "def ger_dfs(cwd, doenca):\n",
    "    ftp = FTP('ftp.datasus.gov.br')\n",
    "    ftp.login('anonymous', 'anonymous@')\n",
    "    dfs = []\n",
    "    ftp.cwd(cwd)\n",
    "    lista = ftp.nlst()\n",
    "\n",
    "    arquivos = [j for j in lista if doenca.upper() in j.upper()]\n",
    "    print(\"Arquivos filtrados:\", arquivos)\n",
    "\n",
    "    for j in arquivos:\n",
    "        try:\n",
    "            buffer = io.BytesIO()\n",
    "            ftp.retrbinary(f'RETR {j}', buffer.write)\n",
    "            buffer.seek(0)\n",
    "\n",
    "            with open(f'{j}.dbc', 'wb') as f:\n",
    "                f.write(buffer.getvalue())\n",
    "\n",
    "            arquivo = f'{j}.dbc'\n",
    "            dbf_arquivo = arquivo.replace(\".dbc\", \".dbf\")\n",
    "            datasus_dbc.decompress(arquivo, dbf_arquivo)\n",
    "\n",
    "            # Aqui usamos o parser customizado\n",
    "            records = DBF(dbf_arquivo, encoding=\"latin1\",\n",
    "                          parserclass=CustomFieldParser,\n",
    "                          ignore_missing_memofile=True)\n",
    "\n",
    "            df = pd.DataFrame(iter(records))\n",
    "            dfs.append(df)\n",
    "\n",
    "            print(f\"[OK] {j} -> {df.shape[0]} linhas\")\n",
    "\n",
    "            for ftemp in [arquivo, dbf_arquivo]:\n",
    "                if os.path.exists(ftemp):\n",
    "                    os.remove(ftemp)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] {j} -> {e}\")\n",
    "            continue\n",
    "\n",
    "    ftp.quit()\n",
    "\n",
    "    if dfs:\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"[OK] DataFrame final: {df_final.shape[0]} linhas\")\n",
    "        return df_final\n",
    "    else:\n",
    "        print(\"[ERRO] Nenhum DataFrame válido foi criado.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c0524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos filtrados: ['DENGBR00.dbc', 'DENGBR01.dbc', 'DENGBR02.dbc', 'DENGBR03.dbc', 'DENGBR04.dbc', 'DENGBR05.dbc', 'DENGBR06.dbc', 'DENGBR07.dbc', 'DENGBR08.dbc', 'DENGBR09.dbc', 'DENGBR10.dbc', 'DENGBR11.dbc', 'DENGBR12.dbc', 'DENGBR13.dbc', 'DENGBR14.dbc', 'DENGBR15.dbc', 'DENGBR16.dbc', 'DENGBR17.dbc', 'DENGBR18.dbc', 'DENGBR19.dbc', 'DENGBR20.dbc', 'DENGBR21.dbc', 'DENGBR22.dbc', 'DENGBR23.dbc', 'DENGBR24.dbc']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mger_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/dissemin/publicos/SINAN/DADOS/FINAIS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdeng\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mger_dfs\u001b[39m\u001b[34m(cwd, doenca)\u001b[39m\n\u001b[32m     29\u001b[39m datasus_dbc.decompress(arquivo, dbf_arquivo)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Aqui usamos o parser customizado\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m records = \u001b[43mDBF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbf_arquivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m              \u001b[49m\u001b[43mparserclass\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCustomFieldParser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m              \u001b[49m\u001b[43mignore_missing_memofile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m df = pd.DataFrame(\u001b[38;5;28miter\u001b[39m(records))\n\u001b[32m     37\u001b[39m dfs.append(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Contabilidade PC\\Documents\\fillipe\\datasus\\.venv\\Lib\\site-packages\\dbfread\\dbf.py:79\u001b[39m, in \u001b[36mDBF.__init__\u001b[39m\u001b[34m(self, filename, encoding, ignorecase, lowernames, parserclass, recfactory, load, raw, ignore_missing_memofile, char_decode_errors)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDBF\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m     78\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"DBF table.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, encoding=\u001b[38;5;28;01mNone\u001b[39;00m, ignorecase=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     80\u001b[39m                  lowernames=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     81\u001b[39m                  parserclass=FieldParser,\n\u001b[32m     82\u001b[39m                  recfactory=collections.OrderedDict,\n\u001b[32m     83\u001b[39m                  load=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     84\u001b[39m                  raw=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     85\u001b[39m                  ignore_missing_memofile=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     86\u001b[39m                  char_decode_errors=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     88\u001b[39m         \u001b[38;5;28mself\u001b[39m.encoding = encoding\n\u001b[32m     89\u001b[39m         \u001b[38;5;28mself\u001b[39m.ignorecase = ignorecase\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = ger_dfs('/dissemin/publicos/SINAN/DADOS/FINAIS', 'deng')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
